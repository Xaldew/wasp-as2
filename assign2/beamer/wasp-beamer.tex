\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyvrb}
\title{Deep Learning on the IMDB Dataset}
\date{WASP Deep Learning}
\author[Agents 47]{The Hitmen --- Agents 47}

%\usepackage{enumitem}

\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usepackage{pgfplotstable}
\usepgfplotslibrary{groupplots}

\newlength\figureheight
\newlength\figurewidth

\usetheme{wasp}

\graphicspath{{./graphics/}}

\usepackage{lipsum}
\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}{Convnets}{Convolutional Neural Networks}

  \begin{itemize}

  \item[Training] Using all data, we reached the best result after around 1 or 2
    epochs to around 93.5 \%. By using less, say 20 \% of the data, training
    improved for up to 4 epochs, but the validation results were considerably
    worse (73 \%). Note however that training with less data is considerably
    faster.

  \item[Batch] Smaller batches train slightly faster, but doesn't otherwise
    affect validation results much (Got 94.1 \% with batch size 64). Too big
    batches may however consume too much memory for efficient training.

  \end{itemize}

\end{frame}

\begin{frame}
  \begin{itemize}

  \item[Choices] Increasing these values can add a significant amount of data to
    the model, and thus \textbf{might} improve the result (93.7 \%), but will
    also take longer to train. Reducing it barely changs the result. (93.1 \%).

  \item[Dropout] 20\% dropout helps slightly: 94.7 \%.  40\% dropout helps a bit
    more: 95.5 \%. Even higher (80 \%) didn't improve the final result, but did
    make the learning process improve for all 10 epochs.

  \item[Archs] Krohn's architecture seems to perform slightly better, but takes
    a little longer to train than Chollet's. Comparing this with a network
    architecture with only dense layers (~88 \%), convolutional architectures
    seem superior.

  \end{itemize}

\end{frame}

\iffalse
\begin{frame}{Convnets}{Training History}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{figure}[ht]
        \centering
        \includegraphics[width=1.2\textwidth]{convnet_training}
      \end{figure}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}[ht]
        \centering
        \includegraphics[width=1.2\textwidth]{convnet_loss}
      \end{figure}

    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Convnets}{Classification Confidence}

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{convnet_histogram}
  \end{figure}

\end{frame}
\fi


\begin{frame}{Convnets}{Why Convolutions for NLP?}

  \begin{itemize}

  \item Intuitively, a 1D convolution would look both forwards and backwards in
    a sentence, mimicking how the natural language refers to earlier or coming
    words.

  \item Training for more than 2 or 3 epochs doesn't appear to be beneficial for
    classification of this data. This does however depend quite a lot on
    hyper-parameter choices.

  \item Best validation score: 95.96 \%.

  \end{itemize}

\end{frame}



\begin{frame}{RNN}{Recurrent Neural Networks}

	Output of the layer is fed back into the layer, which can be seen as a recursion over a time series.

	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{graphics/RNN-unrolled.png}
	\end{figure}

	Testing the impact of stacking layers, 4 models in total are trained. Model 1 contains 1 32-sized RNN layer with 0.2 dropout, Model 2 contains 2 such layers and so on. This is then trained for 5 epochs with 128 batch size. 


\blfootnote{\tiny Image courtesy of Christopher Olah, \url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}}
  
\end{frame}

\begin{frame}{RNN}{Recurrent Neural Networks}
	\begin{figure}
	\centering

	\setlength\figureheight{3cm}
	\setlength\figurewidth{0.5\linewidth}
	
	\input{graphics/RNN_acc.tex}
	\caption{Training (circle) vs Validation (line) accuracy.} 
	\end{figure}
\end{frame}

\begin{frame}{RNN}{Recurrent Neural Networks}
	
	\noindent
	\begin{minipage}{0.45\textwidth}%
		 The different models obtain a ROC value of 
		 \begin{itemize}%[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		 	\item Model 1: 86.34
		 	\item Model 2: 93.49
		 	\item Model 3: 66.09
		 	\item Model 4: 52.38
		 \end{itemize}
	 	Considering the histograms as well, it can be concluded that adding additional layers \emph{can} improve performance but also increases the risk of not converging as seen in Model 3 and 4.
	\end{minipage} \hfill
	\begin{minipage}{0.45\textwidth}%
		\begin{figure}
			\centering
			
			\setlength\figureheight{3cm}
			\setlength\figurewidth{0.5\linewidth}
			
			\input{graphics/RNN_hists.tex}
			\caption{Histograms over score for best validation accuracy.} 
		\end{figure}
	\end{minipage}
\end{frame}


\begin{frame}{LSTM}{Long Short Term Memory}

LSTM networks are type of RNN that tries to remedy the hardness of training long-term dependence in regular RNN network. This is done by introducing more complex connections between the recursions. 

We will here test the impact of different sizes of the LSTM layers and the number of layers, by training 4 models. Model 1 and 2 will have a single layer of size 32 and 64. Model 3 and 4 will have two layers of size 32 and 64. 
  
\end{frame}

\begin{frame}{LSTM}{Long Short Term Memory}
	\noindent
\begin{minipage}{0.45\textwidth}%
	The different models obtain a ROC value of 
	\begin{itemize}%[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		\item Model 1: 91.40
		\item Model 2: 91.75
		\item Model 3: 90.09
		\item Model 4: 92.40
	\end{itemize}
	Making the network bigger increases the accuracy and the well-posendess of the scores. However, the results are still only roughly as good as the best RNN network. 
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}%
	\begin{figure}
		\centering
		
		\setlength\figureheight{3cm}
		\setlength\figurewidth{0.5\linewidth}
		
		\input{graphics/LSTM_hists.tex}
		\caption{Histograms over score for best validation accuracy.} 
	\end{figure}
\end{minipage}
\end{frame}




\begin{frame}{Results}{Convnets, RNN or LSTM}
  \begin{itemize}

  \item Performance wise, convnets, RNN, and LSTM perform roughly the same.

  \item However, convnets are simpler and faster to train so would be the
    supperior choice in this case.

  \item It was easier to achieve better results with LSTM.

  \end{itemize}

  \note{RNN has a shorter memory.}
\end{frame}

\bgroup
\setbeamertemplate{background}{}
\setbeamercolor{background canvas}{bg=black}
% \setbeamertemplate{navigation symbols}{}
\begin{frame}[t,plain]{}{}
  \begin{center}
    {\tiny \textcolor{white}{The End}}
  \end{center}
\end{frame}
\egroup

\end{document}
